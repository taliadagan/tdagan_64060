---
title: 'Assignment #5 ML'
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Packages used
```{r}
library(dplyr)
library(cluster)
library(factoextra)
#install.packages(dendextend)
library(dendextend)
```

#Reading the dataset
```{r}
Myfile <- read.csv("Cereals.csv")
head(Myfile)
summary(Myfile)
```
#Removing and scaling the data
```{r}
#removing missing values
X <- na.omit(Myfile)
Y <- X[,c(-1,-2,-3)]
Myfile <- na.omit(Myfile)

#Scale the data 
XY <- scale(Y)
```

##Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from  single linkage, complete linkage, average linkage, and Ward. Choose the best method. 
```{r}
library(dplyr)
library(cluster)
library(factoextra)
library(dendextend)

#dissimilarity matrix
diss <- dist(XY, method = "euclidean")
#complete linkage 
complete <- hclust(diss, method= "complete")
#average linkage
average <- hclust(diss, method= "average")


#using Agnes to compare the clustering
ag_single <- agnes(XY, method = "single")
ag_complete <- agnes(XY, method = "complete")
ag_average <- agnes(XY, method= "average")
ag_ward <- agnes(XY, method = "ward")

#comparing the coefficents 
ag_single$ac
ag_complete$ac
ag_average$ac
ag_ward$ac

##the best method is Ward because the coefficient is the highest at .9046042 

#plotting the dendrogram
 plot1 <- pltree(ag_ward, cex= .5, hang=1, main="Dendrogram")
```

##How many clusters would you choose? 
```{r}
matrix <- dist(XY, method = "euclidean")
wardclust <- hclust(diss, method = "ward.D2")

#plotting dendrogram
plot(wardclust, cex = .5)
rect.hclust(wardclust, k=5, border= 1:2)

#Using cutree to identify the clusters
clust <- cutree(wardclust, k=5)
table(clust)

#I would choose 5 clusters. This is because after observing the height of the dendrogram, it appears that 5 clusters will work the best.    
```

##Comment on the structure of the clusters and on their stability. Hint: To check stability,  partition the data and see how well clusters formed based on one part apply to the other part. To do this: Cluster partition A 
```{r}
set.seed(100)
new <- Myfile

#removing missing values
cc <- na.omit(new)
cc1 <- cc[,c(-1,-2,-3)]
cc2 <- scale(cc1)
cc3 <- as.data.frame(cc2)

#partition 
n1 <- cc[1:55,]
n2 <- cc[56:74,]

#cluster with agnes
clus1 <- agnes(scale(n1[,-c(1:3)]) , method = "ward")
clus2 <- agnes(scale(n1[,-c(1:3)]) , method = "average")
clus3 <- agnes(scale(n1[,-c(1:3)]) , method = "complete")
clus4 <- agnes(scale(n1[,-c(1:3)]) , method = "single")
cbind( ward = clus1$ac, average = clus2$ac, complete =clus3$ac, single=clus4$ac)

tree <- cutree(clus1, k=5)

#The clusters are stable. 
```

##Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid). 
```{r}
#the centers
x <- as.data.frame(cbind(scale(n1[,-c(1:3)]),tree))
center <- colMeans(x[x$tree==1,])
center2 <- colMeans(x[x$tree==2,])
center3 <- colMeans(x[x$tree==3,])
center4 <- colMeans(x[x$tree==4,])
center5 <- colMeans(x[x$tree==5,])


#binding the centers together
cent <- rbind(center, center2, center3, center4, center5)
cent

```

##Assess how consistent the cluster assignments are compared to the assignments based on all the data
```{r}
#calculating distance 
y<- as.data.frame(rbind(cent[,-14], scale(n2[, -c(1:3)])))
y1 <- get_dist(y)
y2 <- as.matrix(y1)
d <- data.frame(data=seq(1,nrow(n2),1), clusters=rep(0,nrow(n2)))
for( i in 1:nrow(n2)) 
{d[i,2] <- which.min(y2[i+5,1:5])}

d 
y3 <- as.data.frame(cbind(XY,clust))
cbind(y3$clust[56:74], d$clusters)
table(y3$clust[56:74]==d$clusters)

#Assesing the cluster assignments, the data is stable. 
```

##The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis? 
```{r}
cereal <- cbind(cc3, clust)
cereal[cereal$clust==1,]
cereal[cereal$clust==2,]
cereal[cereal$clust==3,]
cereal[cereal$clust==4,]
cereal[cereal$clust==5,]

#Using mean rating calculation to determine the best cluster of healthy cereals
mean(cereal[cereal$clust==1,"rating"])
mean(cereal[cereal$clust==2, "rating"])
mean(cereal[cereal$clust==3, "rating"])
mean(cereal[cereal$clust==4, "rating"])
mean(cereal[cereal$clust==5, "rating"])

#After calculating the mean rating, cluster #1 is the healthier option. This was concluded because cluster #1 has a higher mean rating of 2.242648. Since cluster #1 has a higher mean rating of healthy attributes, the elementary public schools should choose cluster #1. 
```

